{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "# Ранжирование вопросов StackOverflow с помощью векторных представлений слов\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <впишите>\n",
    "\n",
    "## Внимание! Эта версия задания для студентов ВМК МГУ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "В этом задании вы научитесь вычислять близость текстов и применить этот метод для поиска похожих вопросов на [StackOverflow](https://stackoverflow.com).\n",
    "\n",
    "### Используемые библиотеки\n",
    "\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...).\n",
    "- [Numpy](http://www.numpy.org) — библиотека для научных вычислений.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с многими реализованными алгоритмами машинного обучения для анализа данных.\n",
    "- [Nltk](http://www.nltk.org) — инструмент для работы с естественными языками.\n",
    "- [Pytorch](https://pytorch.org/) — инструмент для обучения нейросетей.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве `StackOverflowData.zip`, который состоит из:\n",
    "- `train.tsv` - обучающая выборка. В каждой строке через табуляцию записаны дублирующие друг друга предложения;\n",
    "- `test.tsv` - тестовая выборка. В каждой строке через табуляцию записаны: *<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...*\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import TaskTests\n",
    "\n",
    "task_tests = TaskTests.from_json(path='test_gt.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вектора слов\n",
    "\n",
    "Для решения вам потребуются предобученная модель векторных представлений слов. Используйте [модель эмбеддингов](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), которая была обучена с помощью пакета word2vec на данных Google News (100 миллиардов слов). Модель содержит 300-мерные вектора для 3 миллионов слов и фраз. Вы можете скачать их, запустив блок кода ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_utils import download_google_vectors\n",
    "\n",
    "\n",
    "download_google_vectors(target_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобученные векторные представления слов (2 балла)\n",
    "\n",
    "Скачайте предобученные вектора и загрузите их с помощью функции [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) библиотеки Gensim с параметром *binary=True*. Если суммарный размер векторов больше, чем доступная память, то вы можете загрузите только часть векторов, указав параметр *limit* (рекомендуемое значение: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как пользоваться этими векторами?\n",
    "\n",
    "Как только вы загрузите векторные представления слов в память, убедитесь, что имеете к ним доступ. Сначала вы можете проверить, содержится ли какое-то слово в загруженных эмбедингах:\n",
    "\n",
    "    'word' in wv_embeddings\n",
    "\n",
    "Затем, чтобы получить соответствующий вектор, вы можете использовать оператор доступа по ключу:\n",
    "\n",
    "    wv_embeddings['word']\n",
    "\n",
    "### Проверим, корректны ли векторные представления\n",
    "\n",
    "Чтобы предотвратить возможные ошибки во время первого этапа, можно проверить, что загруженные вектора корректны. Для этого проверьте три пункта:\n",
    "1. Используя метод `.most_similar(positive=..., negative=...)`, найти слово, похожее на `woman`, `king` и непохожее на `man`.\n",
    "2. Используя метод `.doesnt_match(...)`, найти \"белую ворону\" в списке `['breakfast, 'dinner', 'lunch', 'cereal']`.\n",
    "3. Используя метод `.most_similar_to_given(word, [...])`, найти наиболее похожее на `music` слово из списка `['water', 'sound', 'backpack', 'mouse']`.\n",
    "\n",
    "Прокомментируйте полученные результаты: считаете ли вы их верными и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ранжирование вопросов StackOverflow\n",
    "\n",
    "Давайте посмотрим на данные, которые мы будем использовать в рамках задания. Выборка уже разбита на обучающую и тестовую. Все файлы используют табуляцию в качестве разделителя, но они имеют разный формат:\n",
    "\n",
    "- *обучающая* выборка (train.tsv) содержит похожие друг на друга предложения в каждой строке;\n",
    "- *тестовая* выборка (validation.tsv) содержит в каждой строке: *вопрос, похожий вопрос, отрицательный пример 1, отрицательный пример 2, ...*\n",
    "\n",
    "Считайте тестовую (валидационную) выборку. Ответьте на следующие вопросы:\n",
    "1. Сколько пар-дубликатов предоставлено в выборке?\n",
    "2. Сколько в среднем на каждую пару предоставлено отрицательных примеров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "\n",
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_validation_corpus(\n",
    "    num_samples,\n",
    "    amount_of_negatives_per_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Чтобы перейти от отдельных слов к векторным представлениям вопросов, предлагается подсчитать **среднее** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектоора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, embeddings, dim):\n",
    "        \"\"\"\n",
    "            embeddings: word2vec эмбеддинги\n",
    "            dim: размерность word2vec эмбеддингов. Нужна для задания нулего вектора для пустых вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def __call__(self, text, normalize=False):\n",
    "        \"\"\"\n",
    "            Принимает на вход текст и преобразует его в вектор.\n",
    "            \n",
    "            text: строка с вопросом\n",
    "            normalize: при True нужно перед возвращением нормализовать вектор\n",
    "            \n",
    "            returns: вектор вопроса\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(wv_embeddings, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_embedder(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения. Оценим, как будет работать это решение.\n",
    "\n",
    "### Оценка близости текстов\n",
    "\n",
    "В качестве метрики схожести вопросов будем использовать косинусную близость.\n",
    "\n",
    "В валидационном датасете для каждой пары вопросов-дубликатов у нас есть случайные отрицательные примеры. Для каждого триплета (вопрос, дубликат, отрицательные примеры) будет ранжировать с помощью нашей модели и косинусной близости дубликат и отрицательные примеры и смотреть на позицию дубликата.\n",
    "\n",
    "#### Hits@K\n",
    "Довольно простой и легко интерпретируемой метрикой будет количество корректных попаданий дубликата в top \"выдачи\" для какого-то *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)],$$\n",
    "где $q_i$ - $i$-ый вопрос, $dup_i$ - его дубликат, $topK(q_i)$ - первые *K* элементов в ранжированном списке, который выдает наша модель.\n",
    "\n",
    "#### Пример оценок\n",
    "\n",
    "Пусть $N = 1$, вопрос $q_1$ это \"Что такое python\", а его дубликат $dup_1$ это \"Что такое язык python\". Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. *\"Как узнать с++\"*\n",
    "2. *\"Что такое язык python\"*\n",
    "3. *\"Хочу учить Java\"*\n",
    "4. *\"Не понимаю Tensorflow\"*\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [dup_1 \\in top1(q_1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "#### Подсчет метрики Hits@k сразу для нескольких k\n",
    "\n",
    "Чтобы посчитать метрику для нескольких k, не нужно повторно ранжировать нашей моделью вопросы для одного и того же сэмпла. Достаточно посчитать для сэмпла количество **сложных негативов** - отрицательных примеров, оказавшихся в выдаче выше, чем дубликат. Тогда\n",
    "$$Hits@k = \\begin{cases}\n",
    "    1, & N < k \\\\\n",
    "    0, & иначе\n",
    "   \\end{cases},$$\n",
    "где **N** - количество сложных негативов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте подсчет Hits@k для произвольного набора значений k и заданной валидационной выборки, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "    \n",
    "class Scorer:\n",
    "    \n",
    "    def __init__(self, k, embedder):\n",
    "        \"\"\"\n",
    "            k: список значений k, для которых нужно посчитать hits@k\n",
    "            embedder: объект класса Embedder, умеющий преобразовать текст в вектор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def _get_hard_negatives(self, q, pos, negs):\n",
    "        \"\"\"\n",
    "            q: текст вопроса\n",
    "            pos: текст дубликата\n",
    "            negs: список из текстов случайных вопросов\n",
    "            \n",
    "            result: количество сложных отрицательных примеров, оказавшихся выше положительного\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __call__(self, samples, verbose=False):\n",
    "        \"\"\"\n",
    "            samples: список из списков вида [q, pos, neg1, neg2, ...]. Наша валидационная выборка\n",
    "            verbose: выводить progressbar подсчета метрики с помощью tqdm\n",
    "            \n",
    "            result: словарь вида {k: hits@k}\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(validation, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_scorer(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Как вы могли заметить, мы имеем дело с сырыми данными. Это означает, что там присутствует много опечаток, спецсимволов и заглавных букв. В нашем случае это все может привести к ситуации, когда для данных токенов нет предобученных векторов. Поэтому необходима предобработка.\n",
    "\n",
    "Вам требуется:\n",
    "- Перевести символы в нижний регистр;\n",
    "- Заменить символы пунктуации и всевозможные плохие символы на пробелы;\n",
    "- Удалить стопслова.\n",
    "- Удалить слова с длиной меньше трех букв\n",
    "\n",
    "Реализуйте предобработку текста, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "    \n",
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, characters, min_word_length=0, stopwords=None):\n",
    "        \"\"\"\n",
    "            characters: список плохих символов\n",
    "            min_word_length: минимальная допустимая длина для слов\n",
    "            stopwords: множество фоновых слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст для обработки\n",
    "            \n",
    "            returns: обработанный текст\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_text_preprocessor(TextPreprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Множество фоновых слов можно взять из **nltk** с помощью `nltk.corpus.stopwords.words`, выкидываемые плохие символы и пунктуацию следует подобрать самостоятельно.\n",
    "\n",
    "Обработайте текст и продемонстрируйте улучшение качества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение **hits@500** $\\geqslant 0.82$ до предобработки текста и $\\geqslant 0.85$ после предобработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Представления для неизвестных слов. (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, что получить представления для неизвестного слова, воспользуемся следующим подходом:\n",
    "    \n",
    "1. Будем восстанавливать эмбеддинг неизвестного слова как сумму эмбеддингов буквенных триграмм. Например, слово where должно представляться суммой триграмм #wh, whe, her, ere, re#\n",
    "\n",
    "2. В качестве обучающих данных будем использовать слова, для которых есть эмбеддинг в модели. Будем обучать эмбеддинги триграмм по выборке эмбеддингов с помощью функционала MSE:\n",
    "\n",
    "$$L = \\sum_{w \\in W_{known}}\\| f_{\\theta}(w) - v_w \\|^2 \\to \\min_{\\theta}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $W_{known}$ — множество известных модели слов\n",
    "* $f_{\\theta}(w)$ — сумма эмбеддингов триграмм слова $w$\n",
    "* $v_w$ — эмбеддинг слова $w$\n",
    "* $\\theta$ — веса эмбеддингов триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание триграммного токенизатора\n",
    "\n",
    "Для начала, нам нужно:\n",
    "1. Пройтись по известным в word2vec словам и составить множество триграмм, для которых будем обучать векторы\n",
    "2. Составить маппинг из триграмм в индексы\n",
    "3. Реализовать преобразование произвольного слова в список триграмм\n",
    "4. Реализовать преобразование произвольного слова в список индексов триграмм\n",
    "\n",
    "Для реализации всех этих пунктов предлагается использовать шаблон, приведенный ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramTokenizer:\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "            Формируем множество всевозможных триграмм, встречающихся в словах из words.\n",
    "            Делаем маппинг триграмм в индексы.\n",
    "            \n",
    "            words: список слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: колчиество триграмм, для которых мы завели индекс.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_trigrams(word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список триграмм для word\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список индексов триграмм для слова word, которые нашлись в маппинге\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_trigram_tokenizer(TrigramTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания токенизатора используйте обработанный с помощью TextProcessor текст. \n",
    "\n",
    "**Важно:** в токенизатор нужно подавать только слова, известные word2vec'у."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета с w2v векторами и списками индексов триграмм\n",
    "\n",
    "Мы будем обучать триграммную модель в нейросетевом фреймворке pytorch. Для этого нам нужно создать свой датасет.\n",
    "\n",
    "Он должен:\n",
    "1. Принимать список слов, word2vec и уже созданный триграммный токенизатор.\n",
    "2. Выдавать пары вида (эмбеддинг для слова из word2vec, список индексов триграмм для этого слова)\n",
    "\n",
    "Реализовать датасет нужно в шаблоне, приведенном ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainTrigramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, w2v_embeddings, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Формируем выборку для обучения триграммной модели.\n",
    "            ЗАРАНЕЕ считаем маппинг в список индексов для всех известных в word2vec слов.\n",
    "            \n",
    "            vocab: список слов\n",
    "            w2v_embeddings: no comments\n",
    "            tri_tokenizer: токенизатор триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает количество слов, вошедших в маппинг (размер словаря)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: w2v эмбеддинг для idx-го слова в датасете, список соответствующих ему триграмм (тензоры)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "ds = TrainTrigramDataset(w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_dataset(ds, w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание DataLoader'а и Collator'а\n",
    "\n",
    "Нас интересуют в первую очередь четыре параметра при создании DataLoader:\n",
    "1. Датасет. Реализует интерфейс массива - можно узнать длину и получить элемент с индексом, меньшим длины.\n",
    "2. batch_size. Задает размера батча (количества сэмплов, идущих одновременно в модель).\n",
    "3. shuffle. При shuffle == True каждую эпоху при итерировании по даталоадеру мы будем получать сэмплы в произвольном порядке.\n",
    "4. collate_fn. Этот параметр позволяет задать кастомную логику \"склеивания\" сэмплов из датасета в батч.\n",
    "\n",
    "В качестве модели мы будем использовать слой **torch.nn.EmbeddingBag**. Он принимает на вход список индексов и список сдвигов, начинающийся с нуля.\n",
    "\n",
    "Нужно наш список списков индексов триграмм превратить в соответствующий формат, преобразовать векторы слов и два списка (индексов и сдвигов) в pytorch тензоры (torch.tensor).\n",
    "\n",
    "Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из элементов датасета, e.g. [ds[i] for i in [2, 3, 1, 15]]\n",
    "        \n",
    "        returns: w2v эмбеддинги, индексы триграмм, сдвиги для триграмм\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_dataloader(ds, collate_fn, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели\n",
    "\n",
    "При создании модели мы обычно наследуемся от **torch.nn.Module** и создаем нужные нам слои как атрибуты объекта нашего класса.\n",
    "\n",
    "В данном случае предлагается для формирования эмбеддингов использовать **torch.nn.EmbeddingBag**.\n",
    "\n",
    "Реализуйте предложенный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TrigramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество триграмм, для которых обучаются эмбеддинги\n",
    "            embedding_dim: размерность эмбеддингов триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        \"\"\"\n",
    "            returns: размерность эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    @property\n",
    "    def num_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: количество эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def forward(self, trigrams, offsets):\n",
    "        \"\"\"\n",
    "            trigrams: список индексов триграмм (тензор)\n",
    "            offsets: список сдвигов (тензор)\n",
    "            \n",
    "            returns: эмбеддинги слов, составленные из триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_trigram_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание пайплайна обучения\n",
    "\n",
    "Далее необходимо совместить все наработки в единый пайплайн обучения, добавив также критерий для оптимизации и оптимизатор. \n",
    "\n",
    "Предлагается:\n",
    "\n",
    "1. В качестве оптимизатора использовать Adam (можно попробовать подобрать learning rate / weight decay)\n",
    "2. В качестве критерия оптимизации взять nn.MSELoss (можно также закодить лосс самому)\n",
    "3. Для даталоадера выбрать небольшой батч сайз (32, 64, 128, 256)\n",
    "4. Десяти эпох должно быть достаточно для хорошего качества\n",
    "\n",
    "Реализуйте предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        \"\"\"\n",
    "            model: триграммная модель\n",
    "            criterion: функционал ошибки, принимает на вход w2v эмбеддинги и триграммные эмбеддинги\n",
    "            optimizer: оптимизатор для модели\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            Делаем один проход по даталоадеру, с бэкпропом\n",
    "            \n",
    "            dataloader: даталоадер с тренировочными данными\n",
    "            \n",
    "            returns: лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def train(self, dataloader, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloader: тренировочный даталоадер\n",
    "            n_epochs: количество эпох\n",
    "            verbose: выводить лосс каждую эпоху или нет\n",
    "            \n",
    "            returns: список лоссов\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train_step(dataloader)\n",
    "            losses.append(loss)\n",
    "            if verbose:\n",
    "                print(f'epoch: {epoch + 1:>2}, loss: {loss:.4f}, time: {time.time() - start:.4f}')\n",
    "        return losses\n",
    "\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение векторов неизвестных слов. Инференс модели\n",
    "\n",
    "Теперь, когда мы обучили модель, нам необходимо применить её для всех неизвестных слов, т.е. получить для них эмбеддинги.\n",
    "\n",
    "Т.к. для этих слов у нас нет word2vec эмбеддингов, то dataset и collator для обучения не подходят для инференса. Необходимо реализовать датасет и коллатор для инференса по следующим шаблонам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTrigramDataset:\n",
    "    \n",
    "    def __init__(self, vocab, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Датасет с неизвестными словами\n",
    "            \n",
    "            vocab: список слов\n",
    "            tri_tokenizer: триграммный токенизатор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def __len__(self):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    \n",
    "def inference_collate_fn(trigrams):\n",
    "    \"\"\"\n",
    "        trigrams: список списков индексов триграмм\n",
    "        \n",
    "        returns: список индексов, список сдвигов триграмм\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть всё необходимое, чтобы осуществить инференс. Не забудь перед инференсом перевести модель в режим эвала (**model.eval**), а также использовать контекстный менеджер **torch.no_grad**.\n",
    "\n",
    "После инференса сформируйте словарь из известных в word2vec слов и их эмбеддингов, затем дополните его эмбеддингами для неизвестных слов, полученными после инференса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя **Scorer** и **Embedder**, получите новые значения метрик для валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.89$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Обучение векторных представлений для целевой задачи. (4 баллов)\n",
    "\n",
    "Предполагается, что в этой части используются TextPreprocessor, Embedder, Scorer из предыдущих частей.\n",
    "\n",
    "Для обучения на целевую задачу нам понадобится обучающая выборка. Считайте её с диска, предобработайте текст вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for questions in tqdm.tqdm(read_corpus('data/train.tsv')):\n",
    "    train.append([text_preprocessor(text) for text in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо создать **токенизатор для текста** - составить словарь и сделать маппинг из слов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "            vocab: множество слов, встретившихся в обучающей выборке\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: количество слов в словаре\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст\n",
    "            \n",
    "            returns: список индексов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление словаря и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам также понадобится **новый датасет для обучения**. Для применения метода NT-Exent нам не нужно \"майнить негативы\", поэтому датасет надо сформировать как массив из пар-дубликатов.\n",
    "\n",
    "Так как данные в обучающей выборке содержат множества дубликатов (т.е. все дубликаты сгруппированы в списки), есть несколько способов сформировать итоговый датасет:\n",
    "1. Оставить из каждого множества дубликатов какие-нибудь случайные два (или просто первые два вопроса)\n",
    "2. Для первого вопроса в множестве взять все остальные как дубликаты (N вопросов-дубликатов - N-1 пара). Тогда мы увидим каждый вопрос хотя бы один раз при обучении\n",
    "3. Составить всевозможные уникальные пары-дубликаты из этих множеств (т.е. первый вопрос и все остальные вопросы, второй вопрос и все остальные, кроме первого).\n",
    "\n",
    "Каждый следующий способ, начиная с первого, раздувает выборку по размеру, но возможно дает прирост к качеству решения задачи.\n",
    "\n",
    "Реализуйте выбранный вами подход, используя предолженный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDuplicatesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, question_pairs, tokenizer):\n",
    "        \"\"\"\n",
    "            question_pairs: список из пар вопросов-дубликатов\n",
    "            tokenizer: объект класса TextTokenizer\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: количество пар-дубликатов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: (вопрос, дубликат), idx-ю пару в датасете\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нужно подготовить **даталоадер** (а именно - коллатор для даталоадера) по аналогии со второй частью задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_and_offsets(questions):\n",
    "    \"\"\"\n",
    "        questions: список из токенизированных вопросов\n",
    "        \n",
    "        returns: (ids, offsets), где ids - вытянутый список индексов слов в вопросах из батча, offsets - сдвиги\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из пар токенизированных вопросов-дубликатов [(question, duplicate), ...]\n",
    "        \n",
    "        returns: (question_ids, question_offsets), (duplicate_ids, duplicate_offsets)\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на трейн и валидацию, используя train_test_split, затем **создайте датасеты и даталоадеры** для обучения и валидации. Сколько пар-дубликатов получилось в датасете для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью предложенного шаблона **задайте модель** для преобразования вопросов в векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DssmLikeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество слов, для которых обучаем эмбеддинги\n",
    "            embedding_dim: размерность эмбеддинга\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def forward(self, ids, offsets):\n",
    "        \"\"\"\n",
    "            ids: вытянутая посл-ть индексов слов вопросов, попавших в батч\n",
    "            offsets: сдвиги для вопросов, попавших в батч\n",
    "            \n",
    "            returns: векторы вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Критерий оптимизации** для NTExentLoss выглядит как:\n",
    "\n",
    "$$\\mathcal{L}(Q, D) = -0.5 \\log diag(softmax(QD^T / \\alpha)) - 0.5 \\log diag(softmax(DQ^T / \\alpha)),$$\n",
    "\n",
    "где:\n",
    "* $Q \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги вопросов, \n",
    "* $D \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги соответствующих вопросам дубликатов,\n",
    "* $b$ - количество пар (вопрос, дубликат), $d$ - размерность эмбеддингов, $\\alpha$ - гиперпараметр лосса. \n",
    "* Softmax берется по рядам\n",
    "* Матрицы $Q, D$ содержат нормированные эмбеддинги, т.е. считается именно косинус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTExentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1., eps=1e-8):\n",
    "        \"\"\"\n",
    "            alpha: коэффициент, на который мы делим скоры перед софтмаксом\n",
    "            eps: ||v|| = max(eps, ||v||)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def _normalize(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            \n",
    "            returns: матрица такого же размера, но с нормироваными векторами\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def forward(self, embeddings, positives):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            positives: матрица такого же размера, с позитивами для векторов из матрицы embeddings\n",
    "            \n",
    "            returns: NT-Exent loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте пайплайн** для обучения и валидации, используя предложенный шаблон.\n",
    "\n",
    "Залогируйте с помощью **torch.utils.tensorboard.SummaryWriter** две величины:\n",
    "1. Лосс для каждого батча\n",
    "2. Лосс на валидации для каждой эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            logdir=None, \n",
    "            device=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса DssmModel\n",
    "            optimizer: оптимизатор\n",
    "            criterion: критерий оптимизации\n",
    "            logdir: директория, в которую SummaryWriter должен писать логи\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "            batch: батч из индексов и сдвигов для вопросов и их дубликатов\n",
    "            \n",
    "            returns: посчитанный для батча лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для обучения\n",
    "            \n",
    "            returns: лосс на датасете для обучения\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для валидации\n",
    "            \n",
    "            returns: лосс на валидации\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "    \n",
    "    def train(self, dataloaders, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloaders: словарь вида {'train': train_dataloader, 'eval': eval_dataloader}\n",
    "            n_epochs: количество эпох обучения\n",
    "            verbose: нужно ли выводить каждую эпоху информацию про лоссы\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            \n",
    "            eval_loss = self._eval_step(dataloaders['eval'])\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval/loss', eval_loss, global_step=self._n_epoch)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\n",
    "                    'epoch: {:>2}, train loss: {:.4f}, eval loss: {:.4f}, time: {:.4f}' \\\n",
    "                        .format(epoch + 1, train_loss, eval_loss, time.time() - start)\n",
    "                )\n",
    "                    \n",
    "            self._n_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагается использовать для оптимизации Адам и обучать модель 10-60 эпох.\n",
    "\n",
    "Для этой части задания GPU даёт существенное ускорение при обучении, поэтому стоит по возможности делать обучение с большим batch size'ом и на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось достать из модели обученные под задачу векторы слов, составить маппинг слов в векторы, создать **Embedder** и **Scorer** и провалидировать качество на нашей исходной валидации, которой мы пользовались в первых двух частях.\n",
    "\n",
    "Чтобы достать из модели веса, можно использовать `model._embeddings.weight.cpu().detach().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.98$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная часть (до 3 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый из пунктов при успешном выполнении гарантирует как минимум один дополнительный балл. Максимум вам будет зачтено три пункта. \n",
    "1. Обучить триграммную модель на косинусную близость вместо евклидового расстояния, получить прирост качества (относительно триграммной модели с MSE)\n",
    "2. Обучить в качестве триграммной модели char-biLSTM вместо мешка векторов триграмм, получить прирост качества (относительно триграммной модели с таким же критерием оптимизации)\n",
    "3. Усложнить модель в части 3, добавить к мешку вектора словесных униграмм также мешок векторов словесных биграмм и мешок векторов буквенных триграмм (сделать модель более похожей на настоящий dssm), получить прирост качества\n",
    "4. Модифицировать модель в части 3 произвольным образом (добавить MLP, нормализации, дропаут, сделать bilstm поверх последовательности векторов слов, трансформер и т.д.), получить прирост качества\n",
    "5. Сделать модель с ранним связыванием (early fusion) - векторы вопросов конкатенируются и проходят через MLP (с возможными модификациями) перед созданием предсказания. Hint: возможно стоит предобучить эмбеддинги слов с помощью NT-Exent перед обучением финальной модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
